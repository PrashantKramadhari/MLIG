{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuZhAf76sNoh"
   },
   "source": [
    "[<img src=\"https://www.scenerepresentations.org/assets/logo/logo+text-negative@2x.png\" alt=\"TODO: make logo work better on Colab background\" width=\"212\"/>](https://www.scenerepresentations.org)\n",
    "\n",
    "[`MIT 6.S980, Machine Learning for Inverse Graphics`](https://www.scenerepresentations.org/courses/inverse-graphics/)\n",
    "# Homework 3: Prior-based Reconstruction\n",
    "\n",
    "### Administrative details\n",
    "This course publishes its assignments as Jupyter notebooks, hosted on Google Colab. This way, you don't need any local development setup. If you're more comfortable with your local dev environment, feel free to download this notebook instead (\"File -> Download -> Download .ipynb\") and edit it in your local environment. Else, make sure you're logged into your Google Account and click \"File -> Save a copy in Drive\". You can then edit your copy of the notebook to complete your assignment. You'll find instructions for submitting your work at the [end of this notebook](#scrollTo=Submission_Instructions).\n",
    "\n",
    "\n",
    "### About this homework\n",
    "\n",
    "In our last unit, we saw how we can use neural networks as \"just another parameterization of continuous functions\". In other words, there was nothing here that one would conventionally call \"Artificial Intelligence\" or even \"Machine Learning\" - we simply fit the neural network to some function values.\n",
    "\n",
    "Now, we will move on to something that would more commonly be referred to as \"Machine Learning\" and \"Artificial Intelligence\": We will use neural networks to make predictions for inputs *unseen* at training time.\n",
    "\n",
    "Technically, we also already did that in our previous assignment: Whenever you queried your occupancy network at a coordinate that wasn't in the training set, you \"generalized\" to an unseen input.\n",
    "\n",
    "However, since these inputs are low-dimensional, we would be compelled to call this behavior \"interpolation\", and are not overly surprised that it works, as we can easily build algorithms that will interpolate some nearest neighbors in the training set and get a similar performance.\n",
    "\n",
    "The key difference now will be that the inputs will be high-dimensional (think >10,000 dimensions). In such high-dimensional spaces, it is *not* straight-forward to hand-craft an algorithm that will assign a reasonable value to an input unobserved at training time.\n",
    "\n",
    "This, then, is what deep learning is, to date, *uniquely* good at, and is the property that has enabled them to revolutionize computer vision and NLP.\n",
    "\n",
    "### Contributors\n",
    "- _Prof. Vincent Sitzmann, Scene Representation Group._\n",
    "- _Prafull Sharma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0sz1OZsoT1R"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFlhNOvjoWqA",
    "outputId": "01cd8b9e-10c1-4e12-e399-9bc83fc809de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Torch version: 1.12.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#!pip install einops\n",
    "\n",
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List, Optional, Tuple, Generator, Dict\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import imageio\n",
    "import os\n",
    "import skimage\n",
    "import h5py\n",
    "import io\n",
    "\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "print(f\"Installed Torch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUnn1RGnr9KR"
   },
   "source": [
    "### Testing your code\n",
    "\n",
    "For your convenience, we have written a little test function, \"check_function\", which will serve as a unit-test for _every function that you will be graded on_. This will allow you a simple, shallow check whether your function does something reasonable.\n",
    "\n",
    "Note that it is **not guaranteed** that you will get full score if all of your functions pass this test - in our grading script, we test your functions much more thoroughly. It is your responsibility to make sure your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mEp8IGhcr8wH"
   },
   "outputs": [],
   "source": [
    "def check_function(test_name, function_name, test_input, test_output):\n",
    "    try:\n",
    "        student_output = function_name(*test_input)\n",
    "    except TypeError as error:\n",
    "        print(\"Function\", test_name, \"has a error and didn't run cleanly. Error:\", error)\n",
    "        return False\n",
    "    if isinstance(student_output, tuple):\n",
    "        student_output = list(student_output)\n",
    "    else:\n",
    "        student_output = [student_output]\n",
    "\n",
    "    for i in range(len(test_output)):\n",
    "        if not torch.allclose(student_output[i], test_output[i], rtol=1e-03):\n",
    "            print(test_name, \": Your function DOES NOT work.\")\n",
    "            return False\n",
    "    print(test_name, \": Your function works!\")\n",
    "    return True\n",
    "\n",
    "def check_losses(test_name, losses_array, loss_threshold):\n",
    "    average_loss_last5 = np.mean(losses[-5:])\n",
    "    if average_loss_last5 >= loss_threshold:\n",
    "        print(test_name, \": Your function isn't optimized well\")\n",
    "        return False\n",
    "    else:\n",
    "        print(test_name, \": Your function works!\")\n",
    "    return True\n",
    "\n",
    "def check_shape(test_name, your_shape, expected_shape):\n",
    "    if your_shape != expected_shape:\n",
    "        print(test_name, \": Your function DOES NOT output a tensor with expected shape\")\n",
    "        return False\n",
    "    else:\n",
    "        print(test_name, \": Your function outputs the expected shape!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn2DXpgpiXml"
   },
   "source": [
    "# Part 1: Convolutional Auto-Decoders\n",
    "\n",
    "As a training exercise, we will build a convolutional auto-decoder to reconstruct images of MNIST digits.\n",
    "\n",
    "The techniques we will learn will apply 1:1 to Part 2, where we will build auto-decoders for 3D scenes!\n",
    "\n",
    "The MNIST dataset is conveniently available in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "_WWPPAEXiaXp",
    "outputId": "1fb88ff0-f942-40f9-b444-ff02575a3ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = datasets.MNIST('mnist_data', train=True, download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                            torchvision.transforms.Pad(18, fill=0, padding_mode='constant'),\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "# The dataset yields tuples of (image, digit_class)\n",
    "img, digit = next(iter(mnist))\n",
    "\n",
    "print(digit)\n",
    "plt.imshow(img.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWFd3NTKkwK7"
   },
   "source": [
    "## Part 1.1: Writing a Convolutional Decoder\n",
    "\n",
    "First, we will write a convolutional decoder: This is a module that will take in a feature map of size `(batch, in_ch, height, width)` and up-sample it `num_up` times to yield a feature map of size `(batch, out_ch, height*(2^num_up), width*(2^num_up)`.\n",
    "\n",
    "**DISCLAIMER: For upsampling, we will use transpose convolutions. Note that this is, generally, a bad idea - usually, you would use a conditional convolutional network like in the StyleGan v2 decoder, but that is not the focus of this class, and we didn't talk about it, so we're going with this :) Note that that wouldn't change the input-output behavior of this module, it would just replace the transpose convolutions with FiLM / Ada-In conditioned convolutions, but we digress.**\n",
    "\n",
    "Implement the convolutional decoder with transpose convolutions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8UnwFLbk2th",
    "outputId": "706a2b58-bf02-486e-88eb-8e9705d1da15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352660876/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvDecoder : Your function outputs the expected shape!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CoordCat(nn.Module):\n",
    "    \"\"\"\n",
    "    This class takes an input of shape (B, L, ...) and concatenates normalied\n",
    "    coordinates along the latent dimension. This is followed when we need\n",
    "    positional information, such as in the case of CoordConv. We will use this\n",
    "    when we write the decoder in ConvDecoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=\"2d\"):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.mode == \"2d\":\n",
    "            x = torch.linspace(-1, 1, input.shape[2])\n",
    "            y = torch.linspace(-1, 1, input.shape[3])\n",
    "            xy = torch.meshgrid(x, y)\n",
    "            xy = torch.stack(xy, dim=0)\n",
    "            out = xy.repeat(input.shape[0], 1, 1, 1)\n",
    "        else:\n",
    "            x = torch.linspace(-1, 1, input.shape[2])\n",
    "            y = torch.linspace(-1, 1, input.shape[3])\n",
    "            z = torch.linspace(-1, 1, input.shape[4])\n",
    "            xyz = torch.meshgrid(x, y, z)\n",
    "            xyz = torch.stack(xyz, dim=0)\n",
    "            out = xyz.repeat(input.shape[0], 1, 1, 1, 1)\n",
    "        return torch.cat([input, out.to(input.device)], dim=1)\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, in_ch, hidden_ch, num_up, out_ch, mode='2d'):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        In this function, we will construct a convolutional decoder based on the\n",
    "        input mode and other input parameters specifying the input, hidden, and\n",
    "        output channels. num_up specifies the number of layers in the decoder.\n",
    "\n",
    "        For mode is '2d', we use nn.Conv2d to perform the initial and final\n",
    "        convolutions, and nn.Conv3d is used when mode is '3d'.\n",
    "\n",
    "        We use transposed convolutions to upscale the input. We will use\n",
    "        nn.ConvTranspose2d when mode is '2d' and nn.ConvTranspose3d when mode is\n",
    "        '3d'. This is used to decode the input into a 2D or 3D feature tensor.\n",
    "\n",
    "        Note: Do not call '.to(device)' on any modules defined here.\n",
    "              All modules are automatically mounted to device when initiated\n",
    "              and mounted on device.\n",
    "        \"\"\"\n",
    "\n",
    "        if mode=='2d':\n",
    "            conv = nn.Conv2d\n",
    "            transpose_conv = nn.ConvTranspose2d\n",
    "            input_hidden_channels = hidden_ch + 2\n",
    "        elif mode == '3d':\n",
    "            conv = nn.Conv3d\n",
    "            transpose_conv = nn.ConvTranspose3d\n",
    "            input_hidden_channels = hidden_ch + 3\n",
    "\n",
    "        ###\n",
    "        # TODO\n",
    "        # Specify in_conv as a nn.Sequential with (conv, LeakyReLU) where conv\n",
    "        # is the convolution operation defined above. This uses the input\n",
    "        # channels as in_ch and outputs hidden_ch channels with stride of 1 and\n",
    "        # padding specified as 'same' with 'zeros'. Make sure to perform\n",
    "        # LeakyReLU inplace and use negative_slope of 0.2.\n",
    "\n",
    "\n",
    "        self.in_conv = nn.Sequential()\n",
    "        self.in_conv.append(conv(in_ch, hidden_ch, kernel_size=3, stride=1, padding='same'))\n",
    "        self.in_conv.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        # Hidden transposed convolution layers\n",
    "        # One way to define nn.Sequential module is nn.Sequential(*arr) where\n",
    "        # arr is a list containing nn.Module units. Here, we will append\n",
    "        # (CoordCat, transpose_conv, LeakyReLU), 'num_up' times. CoordCat needs\n",
    "        # to mode as the argument. Each of the transpose_conv use\n",
    "        # input_hidden_channels as the number of input channels and input and\n",
    "        # output hidden_ch channels using a kernel size of 3, stride\n",
    "        # of 2, padding as 1, and output_padding as 1.\n",
    "        hidden_conv = []\n",
    "        coord_cat = CoordCat(mode)\n",
    "\n",
    "        for i in range(num_up):\n",
    "            hidden_conv.append(coord_cat)\n",
    "            hidden_conv.append(transpose_conv(input_hidden_channels, hidden_ch, kernel_size=3, stride=2, padding=1,output_padding=1))\n",
    "            hidden_conv.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.hidden_conv = nn.Sequential(*hidden_conv)\n",
    "\n",
    "        # Specify out_conv using conv with hidden_ch input channels and outputting\n",
    "        # out_ch with kernel size as 3, padding as 'same', and padding_mode as\n",
    "        # 'zeros'.\n",
    "\n",
    "        self.out_conv = conv(hidden_ch, out_ch, kernel_size=3, stride=1, padding='same',padding_mode='zeros')\n",
    "\n",
    "        # TODO\n",
    "        ###\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO\n",
    "        # Call in_conv, hidden_conv, and out_conv modules on the input.\n",
    "        x = self.in_conv(input)\n",
    "        x = self.hidden_conv(x)\n",
    "        out = self.out_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test_conv_decoder():\n",
    "    test_in = torch.randn((1, 4, 32, 32))\n",
    "    mod = ConvDecoder(in_ch = 4, hidden_ch = 32, num_up = 3, out_ch = 4)\n",
    "    return mod(test_in)\n",
    "\n",
    "check_shape(\"ConvDecoder\",\n",
    "            test_conv_decoder().shape,\n",
    "            torch.Size([1, 4, 256, 256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5h1MvpAmcFZ"
   },
   "source": [
    "## Part 1.2: Decoding a single latent code to an image.\n",
    "\n",
    "The convolutional decoder enables us to take a single latent code of shape (1, ch, 1, 1) and learn an MLP to upsample it to a full image.\n",
    "\n",
    "Below, write a class that accomplishes this `LatentFeatureGrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m12BExCpm-RW"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LatentFeatureGrid(nn.Module):\n",
    "    def __init__(self, latent_sidelength, latent_ch, num_up, out_ch, mode='2d'):\n",
    "        super().__init__()\n",
    "\n",
    "        if mode == '2d':\n",
    "            latent_shape = 1, latent_ch, latent_sidelength, latent_sidelength\n",
    "        elif mode == '3d':\n",
    "            latent_shape = 1, latent_ch, latent_sidelength, latent_sidelength, latent_sidelength\n",
    "\n",
    "        #######\n",
    "        # TODO\n",
    "        # Generate an nn.Parameter with shape (1, out_dim, *resolution_per_dim)\n",
    "        # called self.latent_grid with small positive random numbers.\n",
    "        if mode == '2d':\n",
    "          self.latent_grid = nn.Parameter(torch.randn(1, latent_ch , latent_sidelength, latent_sidelength))\n",
    "          print(\"2d\")\n",
    "        elif mode == '3d':\n",
    "          self.latent_grid = nn.Parameter(torch.randn(1, out_ch , latent_sidelength, latent_sidelength, latent_sidelength))\n",
    "          print(\"3d\")\n",
    "\n",
    "\n",
    "\n",
    "        # Instatiate decoder as ConvDecoder which takes in latent_ch channels as\n",
    "        # input and output, and the num_up and mode specified in the arguments.\n",
    "\n",
    "        self.decoder = ConvDecoder(in_ch=latent_ch, hidden_ch=latent_ch, out_ch=out_ch, num_up=num_up, mode=mode)\n",
    "\n",
    "        # TODO\n",
    "        ######\n",
    "\n",
    "    def forward(self, input=None):\n",
    "        '''\n",
    "        coordinate: (batch_size, num_points, 2)\n",
    "        '''\n",
    "        # TODO\n",
    "        # return the output of the decoder on input of self.latent_grid.\n",
    "        return self.decoder(self.latent_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7CymGOKnuzX"
   },
   "source": [
    "## Part 1.3: Fitting a single latent to a single image.\n",
    "\n",
    "In homework 1, we fit an image grid to an image grid. Now, we will do something similar: we will fit a low-dimensional feature grid to an image.\n",
    "\n",
    "Let's retrieve a single image from our mnist dataset, and wrap it in a little generator that will always yield exactly that one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t2iW2aopn-9F"
   },
   "outputs": [],
   "source": [
    "image, _ = next(iter(mnist))\n",
    "\n",
    "def image_generator():\n",
    "    while True:\n",
    "        # We have to yield \"none\" as the first entry of the tuple, as our\n",
    "        # Fitting function expects some kind of model_input\n",
    "        yield None, torch.tensor(image, device=device)[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcupwA-d-R-C"
   },
   "source": [
    "We now create the function `fit`, which defines the training loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WGl-H-cXnw1s"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "def to_gpu(ob):\n",
    "    if isinstance(ob, collections.abc.Mapping):\n",
    "        return {k: to_gpu(v) for k, v in ob.items()}\n",
    "    elif isinstance(ob, tuple):\n",
    "        return tuple(to_gpu(k) for k in ob)\n",
    "    elif isinstance(ob, list):\n",
    "        return [to_gpu(k) for k in ob]\n",
    "    else:\n",
    "        try:\n",
    "            return ob.cuda()\n",
    "        except:\n",
    "            return ob\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    data_iterator,\n",
    "    loss_fn,\n",
    "    resolution: Tuple,\n",
    "    optimizer,\n",
    "    plotting_function = None,\n",
    "    steps_til_summary = 500,\n",
    "    total_steps=2001\n",
    "   ):\n",
    "\n",
    "    losses = []\n",
    "    for step in range(total_steps):\n",
    "        # Get the next batch of data and move it to the GPU\n",
    "        model_input, ground_truth = next(data_iterator)\n",
    "        model_input, ground_truth = to_gpu(model_input), to_gpu(ground_truth)\n",
    "\n",
    "        # Compute the MLP output for the given input data and compute the loss\n",
    "        model_output = model(model_input)\n",
    "\n",
    "        # Implement a simple mean-squared-error loss between the\n",
    "        loss = loss_fn(model_output, ground_truth, model) # Note: loss now takes \"model\" as input.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the losses so that we can plot them later\n",
    "        # losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # Every so often, we want to show what our model has learned.\n",
    "        # It would be boring otherwise!\n",
    "        if not step % steps_til_summary:\n",
    "            print(f\"Step {step}: loss = {float(loss.detach().cpu()):.5f}\")\n",
    "\n",
    "            if plotting_function is not None: # Note: we now call the \"plotting function\" instead of hard-coding the plotting here.\n",
    "                plotting_function(model_output, ground_truth, resolution)\n",
    "\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUL5c4X4oRbn"
   },
   "source": [
    "As loss, we will use a simple mse loss for now. Note that it now must accept the \"model\" variable as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QB-kX3ACnro3"
   },
   "outputs": [],
   "source": [
    "def mse_loss(\n",
    "    mlp_out,\n",
    "    gt,\n",
    "    model: nn.Module # We won't be using this one here, but later, we will!\n",
    "    ):\n",
    "    return ((mlp_out - gt)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MvHYQ86-bax"
   },
   "source": [
    "Let's implement the plotting function for this single-image fitting experiment. It will simply take the model output (which in this case will be the image) and plot it next to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DThNSJkh-fsU"
   },
   "outputs": [],
   "source": [
    "def plot_output_ground_truth(model_output, ground_truth, resolution):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), squeeze=False)\n",
    "    axes[0, 0].imshow(model_output.cpu().view(*resolution).detach().numpy())\n",
    "    axes[0, 0].set_title(\"Trained MLP\")\n",
    "    axes[0, 1].imshow(ground_truth.cpu().view(*resolution).detach().numpy())\n",
    "    axes[0, 1].set_title(\"Ground Truth\")\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[0, i].set_axis_off()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE8DnKgMofiH"
   },
   "source": [
    "We instantiate our model, as well as an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afE-G_M9oh2a",
    "outputId": "f76dfd70-e8ca-4861-adc7-6eeed8575ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Define latent_img as LatentFeatureGrid with latent_sidelength as 2, latent_ch\n",
    "# as 128, outputting 1 channel, using 5 upsampling layers (num_up). Mount\n",
    "# the latent_img on the gpu.\n",
    "\n",
    "latent_img = LatentFeatureGrid(latent_sidelength=2, latent_ch=128, num_up=5, out_ch=1).to(device)\n",
    "\n",
    "# Define optim as Adam optimizer that optimizes the parameters of latent_img\n",
    "# with a learning rate of 1e-4\n",
    "\n",
    "optim = torch.optim.Adam(latent_img.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiD4qtbXokSj"
   },
   "source": [
    "And can now call the `fit` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3RyvRmoypTmB",
    "outputId": "56c559cd-5ba1-4dfd-b07c-f0b51aeb3a97"
   },
   "source": [
    "# TODO\n",
    "# Call the fit function using latent_img as the model, image_generator as the\n",
    "# data_iteratorm, mse_loss as the loss_fn. We will optimize with a image resolution\n",
    "# of (64, 64) for 501 steps as fitting an image is relatively quick!\n",
    "# Don't forget to pass the optimize and plot_output_groun_truth for plotting.\n",
    "\n",
    "final_out = fit(model=latent_img, data_iterator=image_generator(),\n",
    "                loss_fn=mse_loss, resolution=(64,64),\n",
    "                optimizer=optim,\n",
    "                plotting_function=plot_output_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5Rtt7UPr2UH"
   },
   "source": [
    "Ok - so far, nothing new. The fact that this works should not surprise you!\n",
    "\n",
    "Now, let's make this more interesting. Let's learn a *space* of MNIST images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TACZ4jiwpANq"
   },
   "outputs": [],
   "source": [
    "del latent_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAJGUpY_sGC4"
   },
   "source": [
    "## Part 1.4: Building a convolutional auto-decoder for images\n",
    "\n",
    "The `LatentImgGrid` module above only stores a *single* latent grid that can only represent a *single* image. This means that we cannot reconstruct an image from incomplete observations - for instance, observing only half the digit.\n",
    "\n",
    "Now, we will learn a *latent space* of digits, such that we can reconstruct a digit from a *partial* observations.\n",
    "\n",
    "We will do that using the *auto-decoder* framework discussed in the lecture. Given a dataset of images $\\{ I_i \\}_{i=1}^N$, we will randomly initialize a corresponding set of latent grids $\\{ \\mathbf{z}_i \\}_{i=1}^N$.\n",
    "\n",
    "We will then solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "    min_{\\{\\mathbf{z}_i\\}} \\sum_i \\| D(\\mathbf{z}_i) - I_i \\|^2\n",
    "$$\n",
    "\n",
    "where $D(\\cdot)$ is our convolutional decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhnQRGhZ1zTE"
   },
   "source": [
    "### Part 1.4.1 Writing an image auto-decoder\n",
    "\n",
    "To build an auto-decoder, we will modify our `LatentFeatureGrid` class from above to have a *set* of latent codes instead of just a single latent code.\n",
    "\n",
    "In a forward pass, our model will now take as input a *scene idx* idx, retrieve the corresponding latent code, and decode that latent code only.\n",
    "\n",
    "Fill in the blanks below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "06s-iQFwsDBZ"
   },
   "outputs": [],
   "source": [
    "class ImageAutoDecoder(nn.Module):\n",
    "    def __init__(self, latent_sidelength, latent_ch, num_up, out_ch, num_latents):\n",
    "        super().__init__()\n",
    "        #######\n",
    "        # TODO\n",
    "\n",
    "        # Generate an nn.Embedding with num_latents each of dimension as\n",
    "        # latent_ch * latent_sidelength * latent_sidelength\n",
    "\n",
    "        self.latents = nn.Embedding(num_latents,latent_ch * latent_sidelength * latent_sidelength)\n",
    "        self.latents.weight.data.normal_\n",
    "\n",
    "        # Initialize the embedding using a normal distribution.\n",
    "        # We can directly do this by calling self.latents.weight.data.normal_\n",
    "        # with the mean of 0 and standard deviation as 1e-1.\n",
    "\n",
    "        ########################## YOUR CODE HERE #############################\n",
    "\n",
    "\n",
    "        # Define decoder as a ConvDecoder which takes in latent_ch channel input\n",
    "        # and outputs a out_ch channel output. We will use num_up upsampling\n",
    "        # layers, each with hidden dimension of 128.\n",
    "\n",
    "        self.decoder = ConvDecoder(in_ch=latent_ch, hidden_ch=128, out_ch=out_ch, num_up=num_up)\n",
    "\n",
    "        # TODO\n",
    "        ######\n",
    "\n",
    "        self.latent_ch = latent_ch\n",
    "        self.latent_sl = latent_sidelength\n",
    "\n",
    "    def forward(self, idcs):\n",
    "        '''\n",
    "        idcs: (batch_size, 1)\n",
    "        '''\n",
    "        # TODO\n",
    "        bs=1\n",
    "        if idcs.shape[0] is not None:\n",
    "          bs=idcs.shape[0]\n",
    "\n",
    "        # Using the input indices 'idcs', get the respective latents\n",
    "        latents = self.latents(idcs)\n",
    "\n",
    "        # Reshape the latents to (batch_size, latent_ch, latent_sidelength, latent_sidelength)\n",
    "        latents = latents.reshape(bs, self.latent_ch, self.latent_sl, self.latent_sl)\n",
    "\n",
    "        # Compute the output of decoder of the latents\n",
    "        outputs = self.decoder(latents)\n",
    "        return outputs, latents\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwwugJi5tkw4"
   },
   "source": [
    "We can now call this module with an input that is a tensor of integer values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2vupeIh-_Qzi",
    "outputId": "099fcc20-ffe4-43cd-e032-b41b993d2d3a"
   },
   "outputs": [],
   "source": [
    "def test_autodecoder_class():\n",
    "    img_ad = ImageAutoDecoder(4, 32, num_up=4, out_ch=1, num_latents=100).cuda()\n",
    "\n",
    "    idx = torch.tensor([0], device=device).int()\n",
    "    img_ad(idx)\n",
    "\n",
    "test_autodecoder_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3e79_xn2l34"
   },
   "source": [
    "### Part 1.4.2 Training a space of MNIST digits\n",
    "\n",
    "Now, let's train a latent space of MNIST digits! First, we'll need to prep a dataset. The dataset will need to yield as model input the index of the respective image, and as ground truth the image.\n",
    "\n",
    "Fill in the blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rLYXg2w92xbj"
   },
   "outputs": [],
   "source": [
    "class MNISTAutoDecData(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, num_images):\n",
    "        self.mnist = datasets.MNIST('mnist_data', train=True, download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    torchvision.transforms.Pad(18, fill=0, padding_mode='constant'),\n",
    "                                    transforms.ToTensor()\n",
    "                                ]))\n",
    "        self.num_images = num_images\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            #######\n",
    "            # TODO\n",
    "            # Get a random integer in range [0, self.num_images)\n",
    "            rand_idx = np.random.randint(0, self.num_images)\n",
    "            #print(rand_idx)\n",
    "\n",
    "            # get the img and digit from mnist dataset\n",
    "            img, digit = self.mnist[rand_idx]\n",
    "            #print(digit)\n",
    "\n",
    "            # TODO\n",
    "            ######\n",
    "\n",
    "            # yield tensor with rand_idx and a tuple of (image, digit)\n",
    "            yield torch.tensor([rand_idx]), (img, digit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcFG_T2X4k-a"
   },
   "source": [
    "We will now decide on the number of images we would like to auto-decode. Let's go with 10,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GOKpwVzrAc2A"
   },
   "outputs": [],
   "source": [
    "num_imgs = 10_000\n",
    "dataset = MNISTAutoDecData(num_imgs)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQaTJBLCAdSx"
   },
   "source": [
    "Let's initialize our auto-decoding model with an according number of latents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YH-hzwm4AlI5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageAutoDecoder(\n",
       "  (latents): Embedding(10000, 64)\n",
       "  (decoder): ConvDecoder(\n",
       "    (in_conv): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (hidden_conv): Sequential(\n",
       "      (0): CoordCat()\n",
       "      (1): ConvTranspose2d(130, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "      (3): CoordCat()\n",
       "      (4): ConvTranspose2d(130, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.2)\n",
       "      (6): CoordCat()\n",
       "      (7): ConvTranspose2d(130, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (8): LeakyReLU(negative_slope=0.2)\n",
       "      (9): CoordCat()\n",
       "      (10): ConvTranspose2d(130, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (11): LeakyReLU(negative_slope=0.2)\n",
       "      (12): CoordCat()\n",
       "      (13): ConvTranspose2d(130, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (14): LeakyReLU(negative_slope=0.2)\n",
       "      (15): CoordCat()\n",
       "      (16): ConvTranspose2d(130, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (17): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (out_conv): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# Create an ImageAutoDecoder with sidelength 1, latent_ch as 64, out_ch as 1,\n",
    "# and number of latents as num_imgs (we want one for each image). Mount the model\n",
    "# on the gpu using .cuda().\n",
    "img_ad = ImageAutoDecoder(latent_sidelength=1, latent_ch=64, num_up=6, out_ch=1, num_latents=num_imgs)\n",
    "img_ad.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykeOpnT4CpBM"
   },
   "source": [
    "Let's test a single batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uQ7jhYAsCrE8"
   },
   "outputs": [],
   "source": [
    "idx, (img, digit) = next(iter(loader))\n",
    "test_out = img_ad(idx.cuda())\n",
    "\n",
    "del test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGsJvanKpL2d"
   },
   "source": [
    "Now, we'll have to write a loss for the auto-decoder. This loss is a bit more involved. It has three parts:\n",
    "1. An image loss - that's just the reconstruction loss.\n",
    "2. An l2 loss on the latents. This encourages the latent space to be smooth by forcing all the latents to stay close to the origin.\n",
    "3. A small l2 loss on the parameters of the convolutional decoder. This prevents overfitting of the decoder.\n",
    "\n",
    "**This is why we need to pass the model as a parameter to the loss function - so that we can enforce a small l2 regularizer on the model parameters.**\n",
    "\n",
    "Fill in the blanks below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U-9EzUs6BHep"
   },
   "outputs": [],
   "source": [
    "def autodecoder_loss(mlp_out, gt, model):\n",
    "\n",
    "    img, latents = mlp_out\n",
    "\n",
    "    # TODO\n",
    "    # Image loss: Compute the mean squared error loss using the img and gt[0]\n",
    "    loss = nn.MSELoss()\n",
    "    img_loss = loss(img, gt[0])\n",
    "\n",
    "    # Latent loss: We want the latents to be small valued which can be achieved\n",
    "    # by adding the mean-square of the values of the latent. This is also called\n",
    "    # sparisity loss as it promotes the values of the latents to be close to 0.\n",
    "    latent_loss =torch.norm(torch.mean(latents), p=2)\n",
    "\n",
    "    # L2 weight regularization: Similarly, we want the weights of the model to\n",
    "    # sparse as well. Loop over all the parameters of the model and add the\n",
    "    # mean-square of the parameters to param_loss.\n",
    "    # Look at model.named_parameters() and make sure not to apply the loss if\n",
    "    # 'latent' is in the name of the parameter.\n",
    "    param_loss = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name[0]!=\"l\":\n",
    "            param_loss += torch.norm(torch.mean(param), p=2)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    return param_loss*1e-2 + latent_loss*1e-2 + img_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq8A-9oVpe-c"
   },
   "source": [
    "We also need to write a new function plotting function, as the `model_output` is now a tuple.\n",
    "\n",
    "All we have to do is to only plot the first part of the tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gm45QONTDG_E"
   },
   "outputs": [],
   "source": [
    "def plot_output_ground_truth(model_output, ground_truth, resolution):\n",
    "    img, _ = model_output\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), squeeze=False)\n",
    "    axes[0, 0].imshow(img[0].cpu().view(*resolution).detach().numpy())\n",
    "    axes[0, 0].set_title(\"Trained MLP\")\n",
    "    axes[0, 1].imshow(ground_truth[0][0].cpu().view(*resolution).detach().numpy())\n",
    "    axes[0, 1].set_title(\"Ground Truth\")\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[0, i].set_axis_off()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVjNy7N4Ao7I"
   },
   "source": [
    "Now, let's fit!\n",
    "\n",
    "**Note that this training takes quite a bit longer than prior experiments (~10 minutes) - that's because this time, our neural network actually has to learn how to compress information!**\n",
    "\n",
    "As you're training, pay attention to how the images look vs. ground truth. One thing you should see is that *the auto-decoder gets the digit right long before it gets the exact shape right*. What I mean by that is that the output might look like a 4, but it looks like a *different* 4 than the ground-truth one. That's a good sign that our latent space is working, and that all the fours lie close to each other in latent space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8_AGCWgx4uoK"
   },
   "source": [
    "# TODO\n",
    "# Define optim as Adam optimizer which optimizes the parameters of img_ad with\n",
    "# a learning rate of 1e-3.\n",
    "optim = torch.optim.Adam(img_ad.parameters(), lr=1e-3)\n",
    "\n",
    "_ = fit(img_ad,\n",
    "        iter(loader),\n",
    "        loss_fn=autodecoder_loss,\n",
    "        resolution=(64, 64),\n",
    "        plotting_function=plot_output_ground_truth,\n",
    "        optimizer=optim,\n",
    "        total_steps=10001\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZz4uwSsD4IB"
   },
   "source": [
    "Hint: the loss should converge to approximately ~ 1e-3 after 10000 steps, and the digits should be clearly legible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjL-StwLDaul"
   },
   "source": [
    "### Part 1.4.3: Reconstructing a partial MNIST digit\n",
    "\n",
    "What have we gained? Two things: we have learned an expressive latent space, and we have gained the ability to reconstruct digits from incomplete observations.\n",
    "\n",
    "Specifically:\n",
    "1. We are given the *left half* of an image.\n",
    "2. We will initialize a new latent code, randomly.\n",
    "3. Freezing the weights of our model, we will only backpropagate into the latent code to make the left half of the images match.\n",
    "\n",
    "First, let's build a loss that only penalizes the left half of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "o1oYFZD_D4Ol"
   },
   "outputs": [],
   "source": [
    "def left_half_loss(mlp_out, gt, model):\n",
    "    img, latents = mlp_out\n",
    "\n",
    "    # TODO\n",
    "    # Compute the pixel-wise squared difference between img and gt[0]\n",
    "    \n",
    "    diff = (torch.abs(img - gt[0]))**2\n",
    "\n",
    "    #print(\"diff shape:\", diff.shape)\n",
    "    # Define the half as the left half of the diff\n",
    "    half = diff[:,:,:32,:32]\n",
    "\n",
    "    #latent_loss =torch.norm(torch.mean(latents), p=2)\n",
    "    \n",
    "    return half.mean() #+ latent_loss*1e-2\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQbLmzoCKJvM"
   },
   "source": [
    "Next, let's re-initialize the latent grid of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZBB26BghLcxe"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    img_ad.latents.weight.data.zero_()\n",
    "    img_ad.latents.weight.data.normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JefBYdCKi-e"
   },
   "source": [
    "Now, we only need to build a optimizer that *only optimizes the latent grid, leaving the convolutional decoder untouched*.\n",
    "\n",
    "**This is why we modified the `fit` function to take the optimizer as input - we now don't want to optimize all the parameters anymore, just a subset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ad.latents.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1IDd89kyJw3B"
   },
   "outputs": [],
   "source": [
    "def return_optimizer():\n",
    "    # TODO\n",
    "    # return an optimizer that optimizes img_ad.latents.weights with a learning\n",
    "    # rate of 1e-4\n",
    "    return torch.optim.Adam([img_ad.latents.weight], lr=1e-4)\n",
    "optim = return_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm80118ALx1k"
   },
   "source": [
    "Let's randomly sample an mnist image and fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "snbdqlQaL0J6"
   },
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    idx, (img, digit) = next(iter(MNISTAutoDecData(num_imgs)))\n",
    "    while True:\n",
    "        yield idx, (img, digit)\n",
    "\n",
    "dataset = test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-SMliplOubLU"
   },
   "source": [
    "# TODO\n",
    "# Call the fit function to fit img_ad using the left_half_loss function using\n",
    "# optim as the optimizer. Train this for 10001 steps.\n",
    "\n",
    "_ = fit(img_ad,\n",
    "        iter(loader),\n",
    "        loss_fn=left_half_loss,\n",
    "        resolution=(64, 64),\n",
    "        plotting_function=plot_output_ground_truth,\n",
    "        optimizer=optim,\n",
    "        total_steps=10001\n",
    "        )\n",
    "\n",
    "img, latent = img_ad(torch.tensor([100]).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq6QiWzZ1hcJ"
   },
   "source": [
    "It works! Note how it completes the rest of the digit, even though it is only observing the left half.\n",
    "\n",
    "Note that the regularization and initialization of the latent codes in this case are critical. For instance, try training the auto-decoder when the latent codes are initialized with a large standard deviation: What you'll find is that you can still fit the training set just fine, but in-painting will *not* work! That's because it's critical that we force the latent space to be compact, i.e., that all the latent codes are close together and tightly packed. L2 regularization of weights and latent codes plays a similar role. If you run into troubles in your project, these are good things to investigate in a toy setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eTogHN6IgYod"
   },
   "outputs": [],
   "source": [
    "del img_ad\n",
    "del mnist\n",
    "del optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5Y16Sn_txD_"
   },
   "source": [
    "# Part 2: Training an auto-decoder of voxelgrid scene representations\n",
    "\n",
    "Next, we will do the same thing just in 3D and with neural rendering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVY3ddFZqcpl"
   },
   "source": [
    "## Part 2.0: Setup\n",
    "\n",
    "First, we'll need a dataset of images of 3D scenes and their camera poses. For that, we'll use the dataset from the paper \"Scene Representation Networks, Sitzmann et al. 2019\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFQkTeuvOtHU",
    "outputId": "7e8bb9c6-0c85-404d-ed66-eadca91b8783"
   },
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"/content/cars_train.hdf5\"):\n",
    "    # Download SRNs-cars dataset\n",
    "    !gdown 1SBjlsizq0sFNkCZxMQh-pNRi0HyFozKb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8oCyH-NQf3D"
   },
   "source": [
    "Next, below, you can find all the multi-view functions again that you already know from Assignments 1 & 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "EQwL_ax3Qj0s",
    "outputId": "4cac08be-1369-4767-e15d-6bdc45bb56b6"
   },
   "outputs": [],
   "source": [
    "def homogenize_points(points: torch.Tensor):\n",
    "    \"\"\"Appends a \"1\" to the coordinates of a (batch of) points of dimension DIM.\n",
    "\n",
    "    Args:\n",
    "        points: points of shape (..., DIM)\n",
    "\n",
    "    Returns:\n",
    "        points_hom: points with appended \"1\" dimension.\n",
    "    \"\"\"\n",
    "    ones = torch.ones_like(points[..., :1], device=points.device)\n",
    "    return torch.cat((points, ones), dim=-1)\n",
    "\n",
    "\n",
    "def homogenize_vecs(vectors: torch.Tensor):\n",
    "    \"\"\"Appends a \"0\" to the coordinates of a (batch of) vectors of dimension DIM.\n",
    "\n",
    "    Args:\n",
    "        vectors: vectors of shape (..., DIM)\n",
    "\n",
    "    Returns:\n",
    "        vectors_hom: points with appended \"0\" dimension.\n",
    "    \"\"\"\n",
    "    zeros = torch.zeros_like(vectors[..., :1], device=vectors.device)\n",
    "    return torch.cat((vectors, zeros), dim=-1)\n",
    "\n",
    "\n",
    "def unproject(\n",
    "    xy_pix: torch.Tensor,\n",
    "    z: torch.Tensor,\n",
    "    intrinsics: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Unproject (lift) 2D pixel coordinates x_pix and per-pixel z coordinate\n",
    "    to 3D points in camera coordinates.\n",
    "\n",
    "    Args:\n",
    "        xy_pix: 2D pixel coordinates of shape (..., 2)\n",
    "        z: per-pixel depth, defined as z coordinate of shape (..., 1)\n",
    "        intrinscis: camera intrinscics of shape (..., 3, 3)\n",
    "\n",
    "    Returns:\n",
    "        xyz_cam: points in 3D camera coordinates.\n",
    "    \"\"\"\n",
    "    xy_pix_hom = homogenize_points(xy_pix)\n",
    "    xyz_cam = torch.einsum('...ij,...kj->...ki', intrinsics.inverse(), xy_pix_hom)\n",
    "    xyz_cam *= z\n",
    "    return xyz_cam\n",
    "\n",
    "\n",
    "def transform_world2cam(xyz_world_hom: torch.Tensor, cam2world: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Transforms points from 3D world coordinates to 3D camera coordinates.\n",
    "\n",
    "    Args:\n",
    "        xyz_world_hom: homogenized 3D points of shape (..., 4)\n",
    "        cam2world: camera pose of shape (..., 4, 4)\n",
    "\n",
    "    Returns:\n",
    "        xyz_cam: points in camera coordinates.\n",
    "    \"\"\"\n",
    "    world2cam = torch.inverse(cam2world)\n",
    "    return transform_rigid(xyz_world_hom, world2cam)\n",
    "\n",
    "\n",
    "def transform_cam2world(xyz_cam_hom: torch.Tensor, cam2world: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Transforms points from 3D world coordinates to 3D camera coordinates.\n",
    "\n",
    "    Args:\n",
    "        xyz_cam_hom: homogenized 3D points of shape (..., 4)\n",
    "        cam2world: camera pose of shape (..., 4, 4)\n",
    "\n",
    "    Returns:\n",
    "        xyz_world: points in camera coordinates.\n",
    "    \"\"\"\n",
    "    return transform_rigid(xyz_cam_hom, cam2world)\n",
    "\n",
    "\n",
    "def transform_rigid(xyz_hom: torch.Tensor, T: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply a rigid-body transform to a (batch of) points / vectors.\n",
    "\n",
    "    Args:\n",
    "        xyz_hom: homogenized 3D points of shape (..., 4)\n",
    "        T: rigid-body transform matrix of shape (..., 4, 4)\n",
    "\n",
    "    Returns:\n",
    "        xyz_trans: transformed points.\n",
    "    \"\"\"\n",
    "    return torch.einsum('...ij,...kj->...ki', T, xyz_hom)\n",
    "\n",
    "\n",
    "def get_unnormalized_cam_ray_directions(xy_pix:torch.Tensor,\n",
    "                                        intrinsics:torch.Tensor) -> torch.Tensor:\n",
    "    return unproject(xy_pix, torch.ones_like(xy_pix[..., :1], device=xy_pix.device),  intrinsics=intrinsics)\n",
    "\n",
    "\n",
    "def get_world_rays(xy_pix: torch.Tensor,\n",
    "                   intrinsics: torch.Tensor,\n",
    "                   cam2world: torch.Tensor,\n",
    "                   ) -> torch.Tensor:\n",
    "    # Get camera origin of camera 1\n",
    "    cam_origin_world = cam2world[..., :3, -1]\n",
    "\n",
    "    # Get ray directions in cam coordinates\n",
    "    ray_dirs_cam = get_unnormalized_cam_ray_directions(xy_pix, intrinsics)\n",
    "\n",
    "    # Homogenize ray directions\n",
    "    rd_cam_hom = homogenize_vecs(ray_dirs_cam)\n",
    "\n",
    "    # Transform ray directions to world coordinates\n",
    "    rd_world_hom = transform_cam2world(rd_cam_hom, cam2world)\n",
    "\n",
    "    # Tile the ray origins to have the same shape as the ray directions.\n",
    "    # Currently, ray origins have shape (batch, 3), while ray directions have shape\n",
    "    cam_origin_world = repeat(cam_origin_world, 'b ch -> b num_rays ch', num_rays=ray_dirs_cam.shape[1])\n",
    "\n",
    "    # Return tuple of cam_origins, ray_world_directions\n",
    "    return cam_origin_world, rd_world_hom[..., :3]\n",
    "\n",
    "\n",
    "def get_opencv_pixel_coordinates(\n",
    "    y_resolution: int,\n",
    "    x_resolution: int,\n",
    "    ):\n",
    "    \"\"\"For an image with y_resolution and x_resolution, return a tensor of pixel coordinates\n",
    "    normalized to lie in [0, 1], with the origin (0, 0) in the top left corner,\n",
    "    the x-axis pointing right, the y-axis pointing down, and the bottom right corner\n",
    "    being at (1, 1).\n",
    "\n",
    "    Returns:\n",
    "        xy_pix: a meshgrid of values from [0, 1] of shape\n",
    "                (y_resolution, x_resolution, 2)\n",
    "    \"\"\"\n",
    "    i, j = torch.meshgrid(torch.linspace(0, 1, steps=x_resolution),\n",
    "                          torch.linspace(0, 1, steps=y_resolution))\n",
    "\n",
    "    xy_pix = torch.stack([i.float(), j.float()], dim=-1).permute(1, 0, 2)\n",
    "    return xy_pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NJbOceRMmlK"
   },
   "source": [
    "## Part 2.1: Dataset class for SRN cars\n",
    "\n",
    "We'll have to write a dataset loader for the SRN dataset.\n",
    "\n",
    "What will this dataset have to yield? Just like in the MNIST example above, it will have to yield *images* and *indices of the corresponding car*, so that the auto-decoder knows which latent it should use. Note that now, there are *several images* for *one latent code*, b/c the latent codes are now matched up with the 3D scenes, not with the images!\n",
    "\n",
    "Further, just like in Assignment 2, our dataset will have to yield *camera extrinsic and intrinsic parameters*, as well as *pixel coordinates*.\n",
    "\n",
    "We bundle up all of these things in a single dictionary of the form\n",
    "\n",
    "```python\n",
    "model_input = {'cam2world': c2ws, # (4,4) cam2world camera pose\n",
    "                'intrinsics': intrinsics, # (3,3) camera intrinsics\n",
    "                'x_pix': x_pix, # (H*W, 2) opencv pixel coordinates\n",
    "                'idx': torch.tensor([idx]) # (1) index of the 3D scene, torch integer tensor\n",
    "                }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "We wrote most of it for you already, but there's a few blanks we'd like you to fill in in the `__iter__` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "qOr_jnHgPdwI"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "def parse_rgb(hdf5_dataset):\n",
    "    s = hdf5_dataset[...].tobytes()\n",
    "    f = io.BytesIO(s)\n",
    "\n",
    "    img = imageio.imread(f)[:, :, :3]\n",
    "    img = skimage.img_as_float32(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def parse_intrinsics(hdf5_dataset):\n",
    "    s = hdf5_dataset[...].tobytes()\n",
    "    s = s.decode('utf-8')\n",
    "\n",
    "    lines = s.split('\\n')\n",
    "    f, cx, cy, _ = map(float, lines[0].split())\n",
    "    full_intrinsic = torch.tensor([[f, 0., cx],\n",
    "                                    [0., f, cy],\n",
    "                                    [0., 0, 1]])\n",
    "\n",
    "    return full_intrinsic\n",
    "\n",
    "\n",
    "def parse_pose(hdf5_dataset):\n",
    "    raw = hdf5_dataset[...]\n",
    "    ba = bytearray(raw)\n",
    "    s = ba.decode('ascii')\n",
    "\n",
    "    lines = s.splitlines()\n",
    "    pose = np.zeros((4, 4), dtype=np.float32)\n",
    "\n",
    "    for i in range(16):\n",
    "        pose[i // 4, i % 4] = lines[0].split(\" \")[i]\n",
    "\n",
    "    pose = torch.from_numpy(pose.squeeze())\n",
    "    return pose\n",
    "\n",
    "\n",
    "class SRNsCars(IterableDataset):\n",
    "    def __init__(self, max_num_instances=None, img_sidelength=None):\n",
    "        self.f = h5py.File('cars_train.hdf5', 'r')\n",
    "        self.instances = sorted(list(self.f.keys()))\n",
    "\n",
    "        self.img_sidelength = img_sidelength\n",
    "\n",
    "        if max_num_instances:\n",
    "            self.instances = self.instances[:max_num_instances]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __iter__(self, override_idx=None):\n",
    "        while True:\n",
    "            if override_idx is not None:\n",
    "                idx = override_idx\n",
    "            else:\n",
    "                idx = random.randint(0, len(self.instances)-1)\n",
    "\n",
    "            key = self.instances[idx]\n",
    "\n",
    "            instance = self.f[key]\n",
    "            rgbs_ds = instance['rgb']\n",
    "            c2ws_ds = instance['pose']\n",
    "\n",
    "            rgb_keys = list(rgbs_ds.keys())\n",
    "            c2w_keys = list(c2ws_ds.keys())\n",
    "\n",
    "            observation_idx = np.random.randint(0, len(rgb_keys))\n",
    "            rgb = parse_rgb( rgbs_ds[rgb_keys[observation_idx]] )\n",
    "\n",
    "            x_pix = get_opencv_pixel_coordinates(*rgb.shape[:2])\n",
    "\n",
    "            # There is a lot of white-space around the cars - we'll thus crop the images a bit:\n",
    "            rgb = rgb[32:-32, 32:-32]\n",
    "            x_pix = x_pix[32:-32, 32:-32]\n",
    "\n",
    "            # Nearest-neighbor downsampling of *both* the\n",
    "            # RGB image and the pixel coordinates. This is better than down-\n",
    "            # sampling RGB only and then generating corresponding pixel coordinates,\n",
    "            # which generates \"fake rays\", i.e., rays that the camera\n",
    "            # didn't actually capture with wrong colors. Instead, this simply picks a\n",
    "            # subset of the \"true\" camera rays.\n",
    "            if self.img_sidelength is not None and rgb.shape[0] != self.img_sidelength:\n",
    "                rgb = resize(rgb,\n",
    "                             (self.img_sidelength, self.img_sidelength),\n",
    "                             anti_aliasing=False,\n",
    "                             order=0)\n",
    "                rgb = torch.from_numpy(rgb)\n",
    "                x_pix = resize(np.array(x_pix),\n",
    "                               (self.img_sidelength, self.img_sidelength),\n",
    "                               anti_aliasing=False,\n",
    "                               order=0)\n",
    "                x_pix = torch.from_numpy(x_pix)\n",
    "\n",
    "\n",
    "            x_pix = rearrange(x_pix, 'i j c -> (i j) c')\n",
    "            c2w = parse_pose( c2ws_ds[c2w_keys[observation_idx]] )\n",
    "\n",
    "            rgb = rearrange(rgb, 'i j c -> (i j) c')\n",
    "\n",
    "            intrinsics = parse_intrinsics( instance['intrinsics.txt'] )\n",
    "            intrinsics[:2, :3] /= 128. # Normalize intrinsics from resolution-specific intrinsics for 128x128\n",
    "\n",
    "            ###\n",
    "            # TODO\n",
    "            # Create a dictionary which contains the following\n",
    "            # 1. the 'cam2world' poses, which we computed as c2w\n",
    "            # 2. the camera 'intrinsics'\n",
    "            # 3. the pixel coordinates, 'x_pix'\n",
    "            # 4. the index of the sampled car, 'idx'\n",
    "            model_input = {\n",
    "                            \"cam2world\":c2w,\n",
    "                            \"intrinsics\":intrinsics,\n",
    "                            \"x_pix\" : x_pix,\n",
    "                            \"idx\" :torch.tensor([idx]),\n",
    "                          }\n",
    "\n",
    "            yield model_input, rgb\n",
    "            # TODO\n",
    "            ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zR3rmt6_H_A"
   },
   "source": [
    "Let's take a look at a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Qf_GLzi0_J-5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 417
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cam2world  tensor([[ 5.2024e-01,  6.5759e-01, -5.4491e-01,  7.0838e-01],\n",
      "        [ 8.5402e-01, -4.0058e-01,  3.3194e-01, -4.3152e-01],\n",
      "        [-8.9407e-08, -6.3805e-01, -7.7000e-01,  1.0010e+00],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  1.0000e+00]])\n",
      "Intrinsics  tensor([[1.0254, 0.0000, 0.5000],\n",
      "        [0.0000, 1.0254, 0.5000],\n",
      "        [0.0000, 0.0000, 1.0000]])\n",
      "Pixel coords shape torch.Size([1024, 2]) tensor(0.7480)\n",
      "Scene idx torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "sl = 32\n",
    "dataset = SRNsCars(img_sidelength=sl)\n",
    "mi, rgb = next(iter(dataset))\n",
    "\n",
    "rgb = rgb.view(sl, sl, 3)\n",
    "\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "\n",
    "print(\"Cam2world \", mi['cam2world'])\n",
    "print(\"Intrinsics \", mi['intrinsics'])\n",
    "print(\"Pixel coords shape\", mi['x_pix'].shape, mi['x_pix'].max())\n",
    "print(\"Scene idx\", mi['idx'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXEY7-FVACuy"
   },
   "source": [
    "***Please make sure that this all makes sense to you.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGdQPWipePYo"
   },
   "source": [
    "## Part 2.2: Latent 3D Grid, Decoder, and Radiance Field\n",
    "\n",
    "We will now write the 3D equivalent of the LatentImgGrid, as well as a Radiance Field that will use that latent voxel grid for rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "a20ESjuEeRfD"
   },
   "outputs": [],
   "source": [
    "class LatentHybridVoxelNeuralField(nn.Module):\n",
    "    def __init__(self, feature_dim, out_dim, mode='bilinear'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "\n",
    "        ######\n",
    "        # TODO\n",
    "        # Initialize the grid as LatentFeatureGrid with side_length of 1,\n",
    "        # latent dimension as the feature_dim, num_up as 5, and out_ch as out_dim.\n",
    "        # Note that in this case, we will use the mode as '3d' for the grid.\n",
    "\n",
    "        self.grid = LatentFeatureGrid(latent_sidelength=1, latent_ch=feature_dim, num_up=5, out_ch=out_dim, mode=\"3d\" )\n",
    "\n",
    "        # Use nn.Sequential to generate an mlp with 2 layers:\n",
    "        # (1) (linear, leaky_relu) with feature_dim inputs and feature_dim outputs\n",
    "        # (2) (linear) with feature_dim inputs and out_dim outputs\n",
    "        mlp = []\n",
    "        mlp.append(nn.Linear(feature_dim, feature_dim))\n",
    "        mlp.append(nn.LeakyReLU())\n",
    "        mlp.append(nn.Linear(feature_dim, out_dim))\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "\n",
    "        # TODO\n",
    "        ######\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, coordinate):\n",
    "        ######\n",
    "        # TODO\n",
    "\n",
    "        bs= coordinate.shape[0]\n",
    "\n",
    "        # Call self.grid to decode latent code into 3D grid.\n",
    "        # Recall that you don't need to pass any input as the decoder is\n",
    "        # evaluated on the latent which is a parameter in the LatentFeatureGrid.\n",
    "        grid = self.grid()\n",
    "\n",
    "        # print(\"Coordinate: \", coordinate.min(), coordinate.max())\n",
    "\n",
    "\n",
    "        # Sample 3D grid at coordinate with grid_sample.\n",
    "        # Remember to add dimensions to make it compatible with grid_sample\n",
    "        coord = coordinate[None, None,:,:]\n",
    "\n",
    "        # TODO\n",
    "        ######\n",
    "\n",
    "        # Grid_sample does not broadcast, so we have to tile the latent grid\n",
    "        # in case we're fitting a single object but have a batch size larger than 1.\n",
    "        if grid.shape[0] != coord.shape[0]:\n",
    "            grid = grid.repeat(coord.shape[0], 1, 1, 1, 1)\n",
    "\n",
    "        ###\n",
    "        # TODO\n",
    "        # Use grid_sample to get the values for the coord in the grid.\n",
    "        # Ensure that you align_corners when using grid_sample.\n",
    "        values = torch.nn.functional.grid_sample(grid, coord,align_corners = True)\n",
    "\n",
    "        # Use squeeze and permute to ensure the shape of values is (B, -1, L)\n",
    "        # where B is the batch size and L is the feature_dim\n",
    "        values = values.reshape(bs,-1,128)\n",
    "\n",
    "        # compute output by evaluating the mlp on the values.\n",
    "        output = self.mlp(values)\n",
    "        return output\n",
    "        # TODO\n",
    "        ######\n",
    "\n",
    "\n",
    "class LatentRadField(nn.Module):\n",
    "    def __init__(self, n_features=128):\n",
    "        super().__init__()\n",
    "\n",
    "        ######\n",
    "        # TODO\n",
    "        # scene_rep is a LatentHybridVoxelNeuralField with n_features as input\n",
    "        # and output feature channels.\n",
    "        self.scene_rep = LatentHybridVoxelNeuralField(n_features, n_features)\n",
    "\n",
    "        # Write a (LeakyReLU, linear, ReLU) MLP \"self.sigma\" that will take as input\n",
    "        # the output of the voxel neural field and output a scalar density. Use\n",
    "        # negative_slope of 0.2 for LeakyReLU.\n",
    "        sigma_mlp = []\n",
    "        sigma_mlp.append(nn.LeakyReLU(0.2))\n",
    "        sigma_mlp.append(nn.Linear(n_features, 1))\n",
    "        sigma_mlp.append(nn.ReLU())\n",
    "        self.sigma = nn.Sequential(*sigma_mlp)\n",
    "\n",
    "        # Write a (LeakyReLU, linear, Sigmoid) MLP \"self.color\" that wll take as input\n",
    "        # the output of the voxel neural field and output a 3 channel density.\n",
    "        # Use negative_slope of 0.2 for LeakyReLU\n",
    "        color_mlp = []\n",
    "        color_mlp.append(nn.LeakyReLU(0.2))\n",
    "        color_mlp.append(nn.Linear(n_features, 3))\n",
    "        color_mlp.append(nn.Sigmoid())\n",
    "        self.radiance = nn.Sequential(*color_mlp)\n",
    "        \n",
    "        #self.radiance = #### YOUR CODE HERE ####\n",
    "\n",
    "        # TODO\n",
    "        ######\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        xyz: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "      '''\n",
    "      Queries the representation for the density and color values\n",
    "      '''\n",
    "      ###\n",
    "      # TODO\n",
    "      # Do a forward pass through the scene representation.\n",
    "      features =self.scene_rep(xyz)\n",
    "\n",
    "      # Do a forward pass through both the self.sigma and self.color MLPs\n",
    "      # to yield sigma and color.\n",
    "      sigma = self.sigma(features)\n",
    "      rad = self.radiance(features)\n",
    "\n",
    "      # TODO\n",
    "      ###\n",
    "\n",
    "      # Return sigma and color.\n",
    "      return sigma, rad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgNxwcaEd-c2"
   },
   "source": [
    "## Part 2.3: Sanity Check: Overfitting a single SRN car\n",
    "\n",
    "Before we will move on to generalization and prior-based reconstruction, it is always a good idea to reconstruct a single example of your training set to make sure camera poses etc. are all correct.\n",
    "\n",
    "We will further use this code as a starting point to build our generalizable architecture.\n",
    "\n",
    "Below, find a more compact implementation of the differentiable renderer, the voxelgrid scene representation, and the radiance field from assignment 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "jb27VNhweAcO"
   },
   "outputs": [],
   "source": [
    "def sample_points_along_rays(\n",
    "    near_depth: float,\n",
    "    far_depth: float,\n",
    "    num_samples: int,\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    device: torch.device\n",
    "):\n",
    "    # Compute a linspace of num_samples depth values beetween near_depth and far_depth.\n",
    "    z_vals = torch.linspace(near_depth, far_depth, num_samples, device=device)\n",
    "\n",
    "    # Using the ray_origins, ray_directions, generate 3D points along\n",
    "    # the camera rays according to the z_vals.\n",
    "    pts = ray_origins[...,None,:] + ray_directions[...,None,:] * z_vals[...,:,None]\n",
    "\n",
    "    return pts, z_vals\n",
    "\n",
    "\n",
    "def volume_integral(\n",
    "    z_vals: torch.tensor,\n",
    "    sigmas: torch.tensor,\n",
    "    radiances: torch.tensor\n",
    ") -> Tuple[torch.tensor, torch.tensor]:\n",
    "\n",
    "    # Compute the deltas in depth between the points.\n",
    "    dists = torch.cat([\n",
    "        z_vals[..., 1:] - z_vals[..., :-1],\n",
    "        torch.broadcast_to(torch.Tensor([1e10]).to(z_vals.device), z_vals[...,:1].shape)\n",
    "        ], -1)\n",
    "\n",
    "    # Compute the alpha values from the densities and the dists.\n",
    "    # Tip: use torch.einsum for a convenient way of multiplying the correct\n",
    "    # dimensions of the sigmas and the dists.\n",
    "    alpha = 1.- torch.exp(-torch.einsum('brzs, z -> brzs', sigmas, dists))\n",
    "\n",
    "    # Compute the Ts from the alpha values. Use torch.cumprod.\n",
    "    Ts = torch.cumprod(1.-alpha + 1e-10, -2)\n",
    "\n",
    "    # Compute the weights from the Ts and the alphas.\n",
    "    weights = alpha * Ts\n",
    "\n",
    "    # Compute the pixel color as the weighted sum of the radiance values.\n",
    "    rgb = torch.einsum('brzs, brzs -> brs', weights, radiances)\n",
    "\n",
    "    # Compute the depths as the weighted sum of z_vals.\n",
    "    # Tip: use torch.einsum for a convenient way of computing the weighted sum,\n",
    "    # without the need to reshape the z_vals.\n",
    "    depth_map = torch.einsum('brzs, z -> brs', weights, z_vals)\n",
    "\n",
    "    return rgb, depth_map, weights\n",
    "\n",
    "\n",
    "class VolumeRenderer(nn.Module):\n",
    "    def __init__(self, near, far, n_samples=32, white_back=True):\n",
    "        super().__init__()\n",
    "        self.near = near\n",
    "        self.far = far\n",
    "        self.n_samples = n_samples\n",
    "        self.white_back = white_back\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        cam2world,\n",
    "        intrinsics,\n",
    "        x_pix,\n",
    "        radiance_field: nn.Module\n",
    "        ) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Takes as inputs ray origins and directions - samples points along the\n",
    "        rays and then calculates the volume rendering integral.\n",
    "\n",
    "        Params:\n",
    "            input_dict: Dictionary with keys 'cam2world', 'intrinsics', and 'x_pix'\n",
    "            radiance_field: nn.Module instance of the radiance field we want to render.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of rgb, depth_map\n",
    "            rgb: for each pixel coordinate x_pix, the color of the respective ray.\n",
    "            depth_map: for each pixel coordinate x_pix, the depth of the respective ray.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, num_rays = x_pix.shape[0], x_pix.shape[1]\n",
    "\n",
    "        # Compute the ray directions in world coordinates.\n",
    "        # Use the function get_world_rays.\n",
    "        ros, rds = get_world_rays(x_pix, intrinsics, cam2world)\n",
    "\n",
    "        # Generate the points along rays and their depth values\n",
    "        # Use the function sample_points_along_rays.\n",
    "        pts, z_vals = sample_points_along_rays(self.near, self.far, self.n_samples,\n",
    "                                                ros, rds, device=x_pix.device)\n",
    "\n",
    "        # Reshape pts to (batch_size, -1, 3).\n",
    "        pts = pts.reshape(batch_size, -1, 3)\n",
    "\n",
    "        # Sample the radiance field with the points along the rays.\n",
    "        sigma, rad = radiance_field(pts)\n",
    "\n",
    "        # Reshape sigma and rad back to (batch_size, num_rays, self.n_samples, -1)\n",
    "        sigma = sigma.view(batch_size, num_rays, self.n_samples, 1)\n",
    "        rad = rad.view(batch_size, num_rays, self.n_samples, 3)\n",
    "\n",
    "        # Compute pixel colors, depths, and weights via the volume integral.\n",
    "        rgb, depth_map, weights = volume_integral(z_vals, sigma, rad)\n",
    "\n",
    "        if self.white_back:\n",
    "            accum = weights.sum(dim=-2)\n",
    "            rgb = rgb + (1. - accum)\n",
    "\n",
    "        return rgb, depth_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z8R-MgS9cWx"
   },
   "source": [
    "Last time, we used a different `fit` function to fit our radiance field, because we had separated the renderer and the scene representation.\n",
    "\n",
    "This time, we will instead write a thin wrapper that combines the `RadianceField` and the `VolumeRenderer` in a single `nn.Module` class so that we can just call it with camera parameters to retrieve an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "rCFs6Yyz-HND"
   },
   "outputs": [],
   "source": [
    "class RadFieldAndRenderer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rf = LatentRadField(n_features=128)\n",
    "        self.renderer = VolumeRenderer(near=1., far=2.5, n_samples=100,\n",
    "                                       white_back=True).cuda()\n",
    "\n",
    "    def forward(self, model_input):\n",
    "        xy_pix = model_input['x_pix']\n",
    "        intrinsics = model_input['intrinsics']\n",
    "        c2w = model_input['cam2world']\n",
    "\n",
    "        rgb, depth = self.renderer(\n",
    "                              c2w,\n",
    "                              intrinsics,\n",
    "                              xy_pix,\n",
    "                              self.rf\n",
    "                              )\n",
    "        return rgb, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YtT5CA5-XFb"
   },
   "source": [
    "As a sanity check, we will fit only *one* of the SRNs cars with this architecture. We first need a plotting function again that plots the model output and the ground truth image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "YA9pGmJc-iu1"
   },
   "outputs": [],
   "source": [
    "def plot_output_ground_truth(model_output, ground_truth, resolution):\n",
    "    img, depth = model_output\n",
    "\n",
    "    img = img[0]\n",
    "    depth = depth[0]\n",
    "    gt = ground_truth[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), squeeze=False)\n",
    "    axes[0, 0].imshow(img.cpu().view(*resolution).detach().numpy())\n",
    "    axes[0, 0].set_title(\"Trained MLP\")\n",
    "    axes[0, 1].imshow(gt.cpu().view(*resolution).detach().numpy())\n",
    "    axes[0, 1].set_title(\"Ground Truth\")\n",
    "\n",
    "    depth = depth.cpu().view(*resolution[:2]).detach().numpy()\n",
    "    axes[0, 2].imshow(depth, cmap='Greys')\n",
    "    axes[0, 2].set_title(\"Depth\")\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[0, i].set_axis_off()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK43JiXZ-5Nm"
   },
   "source": [
    "We'll instantiate the dataset with only *one* car:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "iZ9zYOCCAVj9"
   },
   "outputs": [],
   "source": [
    "img_sl = 64\n",
    "\n",
    "dataset = SRNsCars(max_num_instances=1, img_sidelength=img_sl)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXA-K0J8AbiQ"
   },
   "source": [
    "We'll use a simple mse loss again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "I3rwtJ1uAdBH"
   },
   "outputs": [],
   "source": [
    "def mse_loss(mlp_out, gt, model):\n",
    "    img, depth = mlp_out\n",
    "    return ((img - gt)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW8FTN-eAV79"
   },
   "source": [
    "Let's fit!\n",
    "\n",
    "\n",
    "**Note: Due to the limitations in GPU memory that Google Colab comes with, we have to adjust the following hyper-parameters to save memory: low-resolution voxelgrids, low-resolution images, few samples in the volumetric renderer, and a relatively weak 3D convolutional neural network decoder. If we would run this on a faster & larger GPU, we would get drastically better results!**\n",
    "\n",
    "Note that if you would like, you can either run this notebook locally (on a machine with a better GPU) or pay $10 for the month to get Colab Pro, which gives you access to much faster GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "JrUyikNkgWAa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d\n"
     ]
    }
   ],
   "source": [
    "rf_and_renderer = RadFieldAndRenderer().cuda()\n",
    "optim = torch.optim.Adam(lr=1e-3, params=rf_and_renderer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "DReKzkPx-4il"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 0.43616\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 451,
       "width": 1415
      }
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_and_renderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_sl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_sl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotting_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_output_ground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_til_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, data_iterator, loss_fn, resolution, optimizer, plotting_function, steps_til_summary, total_steps)\u001b[0m\n\u001b[1;32m     34\u001b[0m model_input, ground_truth \u001b[38;5;241m=\u001b[39m to_gpu(model_input), to_gpu(ground_truth)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compute the MLP output for the given input data and compute the loss\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Implement a simple mean-squared-error loss between the\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(model_output, ground_truth, model) \u001b[38;5;66;03m# Note: loss now takes \"model\" as input.\u001b[39;00m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[87], line 13\u001b[0m, in \u001b[0;36mRadFieldAndRenderer.forward\u001b[0;34m(self, model_input)\u001b[0m\n\u001b[1;32m     10\u001b[0m intrinsics \u001b[38;5;241m=\u001b[39m model_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintrinsics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m c2w \u001b[38;5;241m=\u001b[39m model_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcam2world\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m rgb, depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mc2w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mintrinsics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mxy_pix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrf\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rgb, depth\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[86], line 86\u001b[0m, in \u001b[0;36mVolumeRenderer.forward\u001b[0;34m(self, cam2world, intrinsics, x_pix, radiance_field)\u001b[0m\n\u001b[1;32m     82\u001b[0m batch_size, num_rays \u001b[38;5;241m=\u001b[39m x_pix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_pix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Compute the ray directions in world coordinates.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Use the function get_world_rays.\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m ros, rds \u001b[38;5;241m=\u001b[39m \u001b[43mget_world_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintrinsics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam2world\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Generate the points along rays and their depth values\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Use the function sample_points_along_rays.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m pts, z_vals \u001b[38;5;241m=\u001b[39m sample_points_along_rays(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnear, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples,\n\u001b[1;32m     91\u001b[0m                                         ros, rds, device\u001b[38;5;241m=\u001b[39mx_pix\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[82], line 108\u001b[0m, in \u001b[0;36mget_world_rays\u001b[0;34m(xy_pix, intrinsics, cam2world)\u001b[0m\n\u001b[1;32m    105\u001b[0m rd_cam_hom \u001b[38;5;241m=\u001b[39m homogenize_vecs(ray_dirs_cam)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Transform ray directions to world coordinates\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m rd_world_hom \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_cam2world\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrd_cam_hom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam2world\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Tile the ray origins to have the same shape as the ray directions.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Currently, ray origins have shape (batch, 3), while ray directions have shape\u001b[39;00m\n\u001b[1;32m    112\u001b[0m cam_origin_world \u001b[38;5;241m=\u001b[39m repeat(cam_origin_world, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ch -> b num_rays ch\u001b[39m\u001b[38;5;124m'\u001b[39m, num_rays\u001b[38;5;241m=\u001b[39mray_dirs_cam\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[82], line 73\u001b[0m, in \u001b[0;36mtransform_cam2world\u001b[0;34m(xyz_cam_hom, cam2world)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_cam2world\u001b[39m(xyz_cam_hom: torch\u001b[38;5;241m.\u001b[39mTensor, cam2world: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms points from 3D world coordinates to 3D camera coordinates.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        xyz_world: points in camera coordinates.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform_rigid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz_cam_hom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam2world\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[82], line 86\u001b[0m, in \u001b[0;36mtransform_rigid\u001b[0;34m(xyz_hom, T)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_rigid\u001b[39m(xyz_hom: torch\u001b[38;5;241m.\u001b[39mTensor, T: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a rigid-body transform to a (batch of) points / vectors.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m        xyz_trans: transformed points.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m...ij,...kj->...ki\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyz_hom\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_bmm)"
     ]
    }
   ],
   "source": [
    "_ = fit(rf_and_renderer,\n",
    "        iter(data_loader),\n",
    "        loss_fn=mse_loss,\n",
    "        resolution=(img_sl, img_sl, 3),\n",
    "        plotting_function=plot_output_ground_truth,\n",
    "        optimizer=optim,\n",
    "        total_steps=1_001,\n",
    "        steps_til_summary=100\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emxjxq1DNpIM"
   },
   "source": [
    "You should be able to recognize the car after ~1k steps. Note that this reconstrution is likely far from perfect due to our memory and compute constraints. Your output quality would most likely look like this:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?id=1-eVNtUydv72qpbQYnVXc8APGgsfYJ9x9\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "Note that just like with the `LatentFeatureGrid` from above that we used to overfit on a single image, this only fits a *single* latent voxel grid for a *single* 3D car. But what if, instead, we want to learn a *latent space* of these cars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "8jc2Ow4vgSaR"
   },
   "outputs": [],
   "source": [
    "del rf_and_renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xB1-2FU3dK0"
   },
   "source": [
    "## Part 2.4: Writing a torch class for turning single-sample models into auto-decoders\n",
    "\n",
    "Next, we will extend the voxelgrid-conditioned radiance field from the previous assignment to an auto-decoder.\n",
    "\n",
    "For that, we will have to convert the class `RadFieldAndRenderer` into an auto-decoder. What we would like to do is to turn the `self.grid` parameter of the `HyrbidVoxelNeuralField` into an `nn.Embedding` again, so that we can store a *stack* of voxelgrids. Or, we might even consider having these voxelgrids be the output of a convolutional decoder, such that we compress each voxelgrids into a *single* latent.\n",
    "\n",
    "Alas, in the current framework, that would be hard to do, as these classes are nested: the `RadianceField` owns the `HyrbidGroundPlanNeuralField` class which in turn manages the `self.grid` parameter. If we would turn that into an embedding, we would have to modify all the `forward` passes to take an additional `index` parameter, and write a bunch of new code, which might introduce new bugs...\n",
    "\n",
    "It turns out there is an easier way. Pytorch has recently introduced a handy little function called `functional_call` - read through the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.utils.stateless.functional_call.html). This function allows us to call a module's forward pass, as we normally would, *but also pass in a dictionary that maps the name of a parameter to an alternative value that we would like the module to use*!\n",
    "\n",
    "Let's take a look. We will instantiate a new `LatentFeatureGrid` as in part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Pflg8OB25VCF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d\n"
     ]
    }
   ],
   "source": [
    "lfg = LatentFeatureGrid(latent_sidelength=1, latent_ch=128, num_up=5, out_ch=3).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWJ-kW3L5gH0"
   },
   "source": [
    "Let's take a look at its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "SXPjAVYW5h5Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_grid: (1, 128, 1, 1)\n",
      "decoder.in_conv.0.weight: (128, 128, 3, 3)\n",
      "decoder.in_conv.0.bias: (128,)\n",
      "decoder.hidden_conv.1.weight: (130, 128, 3, 3)\n",
      "decoder.hidden_conv.1.bias: (128,)\n",
      "decoder.hidden_conv.4.weight: (130, 128, 3, 3)\n",
      "decoder.hidden_conv.4.bias: (128,)\n",
      "decoder.hidden_conv.7.weight: (130, 128, 3, 3)\n",
      "decoder.hidden_conv.7.bias: (128,)\n",
      "decoder.hidden_conv.10.weight: (130, 128, 3, 3)\n",
      "decoder.hidden_conv.10.bias: (128,)\n",
      "decoder.hidden_conv.13.weight: (130, 128, 3, 3)\n",
      "decoder.hidden_conv.13.bias: (128,)\n",
      "decoder.out_conv.weight: (3, 128, 3, 3)\n",
      "decoder.out_conv.bias: (3,)\n"
     ]
    }
   ],
   "source": [
    "def print_params(module):\n",
    "    for name, param in module.named_parameters():\n",
    "        print(f\"{name}: {tuple(param.shape)}\")\n",
    "\n",
    "print_params(lfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv8fe1T65mHv"
   },
   "source": [
    "`functional_call` now allows us to assemble a dictionary with *alternative parameters* that this module should use in a forward pass!\n",
    "\n",
    "For instance, we can assemble a dictionary with an *alternative* latent_grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "H7eLmFV25zJZ"
   },
   "outputs": [],
   "source": [
    "params = {'latent_grid':torch.rand(1, 128, 1, 1).cuda()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wYQr_-B547c"
   },
   "source": [
    "By simply passing this as the second parameter to `functional_call`, we can force the `LatentGrid` module to use that latent grid instead of its own in its forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "l3GEC2Ps5HwF"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.stateless import functional_call\n",
    "_ = functional_call(lfg, params, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH6BCkFT6GD0"
   },
   "source": [
    "How does that help? It allows us to write a simple wrapper that turns *any* Pytorch Module into an autodecoder! We wrote this one for you, but please do take a look, because you will need it in a second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "ofJj58YV6VKs"
   },
   "outputs": [],
   "source": [
    "class AutoDecoderWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_latents:int, # Number of latent codes for the auto-decoder.\n",
    "        submodule:nn.Module, # nn.Module instance we want to turn into an auto-decoder.\n",
    "        param_name:str # Name of the parameter of the nn.Module that we want to replace with the auto-decoded latents.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        trgt_param = dict(submodule.named_parameters())[param_name]\n",
    "        self.trgt_param_shape = trgt_param.shape[1:]\n",
    "\n",
    "        # nn.Embedding is like a \"list\" of parameters: In the forward pass,\n",
    "        # you can pass a tensor of indices, and this will return the corresponding\n",
    "        # latent.\n",
    "        self.latents = nn.Embedding(num_embeddings=num_latents,\n",
    "                                    embedding_dim=np.prod(self.trgt_param_shape))\n",
    "        self.latents.weight.data.normal_(0, 1e-1)\n",
    "\n",
    "        self.param_name = param_name\n",
    "        self.submodule = submodule\n",
    "\n",
    "    def forward(self,\n",
    "                inputs):\n",
    "        '''\n",
    "        inputs: dictionary. Expected to have key \"idx\" for the latent idcs, as well as other inputs for the sub-module.\n",
    "        '''\n",
    "\n",
    "        #######\n",
    "        # TODO\n",
    "\n",
    "        # Retrieve the dictionary entry \"idx\" from the input dictionary\n",
    "        latent_idcs = inputs[\"idx\"]\n",
    "\n",
    "        batch_size = latent_idcs.shape[0]\n",
    "\n",
    "        # Use the \"idx\" variable to index into the self.latents nn.Embedding module\n",
    "        # and reshape it as (batch_size, target_param_shape[0], ..., target_param_shape[-1])\n",
    "        params = self.latents(latent_idcs).reshape(batch_size,self.trgt_param_shape[0], self.trgt_param_shape[1],self.trgt_param_shape[2] )\n",
    "\n",
    "        # Build a parameter dictionary where self.param_name key holds param as value.\n",
    "        param_dict = {\n",
    "                        self.param_name:params\n",
    "                    }\n",
    "\n",
    "        # call functional_call using the submodule which is the network, the\n",
    "        # param_dict, and the inputs as arguments,\n",
    "        output = functional_call(self.submodule, param_dict, inputs)\n",
    "\n",
    "        # TODO\n",
    "        #######\n",
    "\n",
    "        # return the output of the functional call and params\n",
    "        return output, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfSDXgrZ6X4S"
   },
   "source": [
    "Now, we can use this wrapper to turn `LatentFeatureGrid` into an auto-decoder *without re-writing any of it*. Let's see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "D4f5ria27ARH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d\n"
     ]
    }
   ],
   "source": [
    "lfg = LatentFeatureGrid(latent_sidelength=1, latent_ch=128, num_up=5, out_ch=3)\n",
    "lfg_ad = AutoDecoderWrapper(10_000,\n",
    "                            lfg,\n",
    "                            param_name='latent_grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwpLGlD67Jg6"
   },
   "source": [
    "The object `lfg_ad` is now an auto-decoder - that means that we can pass it a tensor of integers, it will retrieve the corresponding latent feature grids, and call the object `lfg`, an instance of `LatentFeatureGrid`, with these feature grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "2JwMwuIx7ZdK"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape(): argument 'shape' must be tuple of ints, but found element of type ellipsis at pos 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_idcs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mlfg_ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[106], line 38\u001b[0m, in \u001b[0;36mAutoDecoderWrapper.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m latent_idcs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Use the \"idx\" variable to index into the self.latents nn.Embedding module\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# and reshape it as (batch_size, target_param_shape[0], ..., target_param_shape[-1])\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_idcs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrgt_param_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrgt_param_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Build a parameter dictionary where self.param_name key holds param as value.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m param_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_name:params\n\u001b[1;32m     43\u001b[0m             }\n",
      "\u001b[0;31mTypeError\u001b[0m: reshape(): argument 'shape' must be tuple of ints, but found element of type ellipsis at pos 3"
     ]
    }
   ],
   "source": [
    "test_idcs = torch.randint(0, 100, size=(100, 1))\n",
    "_ = lfg_ad({\"idx\":torch.tensor([0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "4i0RWlhKgOnJ"
   },
   "outputs": [],
   "source": [
    "del lfg_ad\n",
    "del lfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HaGZ7My7u2M"
   },
   "source": [
    "## Part 2.5: Turning our latent voxel-grid radiance field into an auto-decoder\n",
    "\n",
    "Equipped with this handy tool, we can now easily build an auto-decoder for our latent radiance fields.\n",
    "\n",
    "First, let's instantiate our dataset and determine how many cars we want to train on. Let's say, 1,000:\n",
    "\n",
    "**Note: For this to work, we have to run a reasonable batch size on the order of ~32. Thus, we will reduce the image resolution to save memory - a resolution of 32 really isn't much, but of course, this works with higher resolutions on larger GPUs :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "B9FWYagmw4PR"
   },
   "outputs": [],
   "source": [
    "# Hint: While you are debugging, set the number of scenes to \"5\".\n",
    "# While you are debugging, set the batch size to \"2\".\n",
    "img_sl = 32\n",
    "num_scenes = 100 # To make this faster for you, we'll only train on 100 cars. This is of course not really enough to learn a good prior, but it'll be much, much faster to train...\n",
    "batch_size = 32\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "dataset = SRNsCars(max_num_instances=num_scenes, img_sidelength=img_sl)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3wbs3cQw2OF"
   },
   "source": [
    "Let's instantiate the model for a *single* scene and print its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "1OYm8uJxto09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d\n",
      "rf.scene_rep.grid.latent_grid: (1, 128, 1, 1, 1)\n",
      "rf.scene_rep.grid.decoder.in_conv.0.weight: (128, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.in_conv.0.bias: (128,)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.1.weight: (131, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.1.bias: (128,)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.4.weight: (131, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.4.bias: (128,)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.7.weight: (131, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.7.bias: (128,)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.10.weight: (131, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.10.bias: (128,)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.13.weight: (131, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.hidden_conv.13.bias: (128,)\n",
      "rf.scene_rep.grid.decoder.out_conv.weight: (128, 128, 3, 3, 3)\n",
      "rf.scene_rep.grid.decoder.out_conv.bias: (128,)\n",
      "rf.scene_rep.mlp.0.weight: (128, 128)\n",
      "rf.scene_rep.mlp.0.bias: (128,)\n",
      "rf.scene_rep.mlp.2.weight: (128, 128)\n",
      "rf.scene_rep.mlp.2.bias: (128,)\n",
      "rf.sigma.1.weight: (1, 128)\n",
      "rf.sigma.1.bias: (1,)\n",
      "rf.radiance.1.weight: (3, 128)\n",
      "rf.radiance.1.bias: (3,)\n"
     ]
    }
   ],
   "source": [
    "srn_rf = RadFieldAndRenderer()\n",
    "print_params(srn_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8qJSSLatvCX"
   },
   "source": [
    "Let's use our AutoDecoderWrapper to turn this into an auto-decoder! Remember to pass in the `num_scenes` parameter.\n",
    "\n",
    "Below, fill in the blanks to turn this module into an auto-decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "bSRUFZqrDuSx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents.weight: (100, 128)\n",
      "submodule.rf.scene_rep.grid.latent_grid: (1, 128, 1, 1, 1)\n",
      "submodule.rf.scene_rep.grid.decoder.in_conv.0.weight: (128, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.in_conv.0.bias: (128,)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.1.weight: (131, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.1.bias: (128,)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.4.weight: (131, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.4.bias: (128,)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.7.weight: (131, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.7.bias: (128,)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.10.weight: (131, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.10.bias: (128,)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.13.weight: (131, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.hidden_conv.13.bias: (128,)\n",
      "submodule.rf.scene_rep.grid.decoder.out_conv.weight: (128, 128, 3, 3, 3)\n",
      "submodule.rf.scene_rep.grid.decoder.out_conv.bias: (128,)\n",
      "submodule.rf.scene_rep.mlp.0.weight: (128, 128)\n",
      "submodule.rf.scene_rep.mlp.0.bias: (128,)\n",
      "submodule.rf.scene_rep.mlp.2.weight: (128, 128)\n",
      "submodule.rf.scene_rep.mlp.2.bias: (128,)\n",
      "submodule.rf.sigma.1.weight: (1, 128)\n",
      "submodule.rf.sigma.1.bias: (1,)\n",
      "submodule.rf.radiance.1.weight: (3, 128)\n",
      "submodule.rf.radiance.1.bias: (3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# Define srns_ad as AutoDecoderWrapper with num_latents equal to num_scenes,\n",
    "# submodule as srn_rf, and param_name as rf.scene_rep.grid.latent_grid)\n",
    "\n",
    "srns_ad = AutoDecoderWrapper(num_latents=num_scenes,submodule= srn_rf, param_name=\"rf.scene_rep.grid.latent_grid\")\n",
    "\n",
    "print_params(srns_ad)\n",
    "\n",
    "num_scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpX-kQ1atyZ_"
   },
   "source": [
    "Let's instantiate our dataset and do a forward pass as a sanity check!\n",
    "\n",
    "**Note that the previous and this cell are both executed on CPU - that's because it's often much easier to debug certain problems on CPU than on GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "z-noTEn5DVGb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam2world torch.Size([32, 4, 4])\n",
      "intrinsics torch.Size([32, 3, 3])\n",
      "x_pix torch.Size([32, 1024, 2])\n",
      "idx torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 128, 3, 3, 3], expected input[1, 32, 128, 1, 1] to have 128 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m mi\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key, val\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 7\u001b[0m test_out \u001b[38;5;241m=\u001b[39m \u001b[43msrns_ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[98], line 47\u001b[0m, in \u001b[0;36mAutoDecoderWrapper.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m param_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_name:params\n\u001b[1;32m     43\u001b[0m             }\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# call functional_call using the submodule which is the network, the\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# param_dict, and the inputs as arguments,\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#######\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# return the output of the functional call and params\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, params\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/utils/stateless.py:141\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         out \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[87], line 13\u001b[0m, in \u001b[0;36mRadFieldAndRenderer.forward\u001b[0;34m(self, model_input)\u001b[0m\n\u001b[1;32m     10\u001b[0m intrinsics \u001b[38;5;241m=\u001b[39m model_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintrinsics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m c2w \u001b[38;5;241m=\u001b[39m model_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcam2world\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m rgb, depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mc2w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mintrinsics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mxy_pix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrf\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rgb, depth\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[86], line 97\u001b[0m, in \u001b[0;36mVolumeRenderer.forward\u001b[0;34m(self, cam2world, intrinsics, x_pix, radiance_field)\u001b[0m\n\u001b[1;32m     94\u001b[0m pts \u001b[38;5;241m=\u001b[39m pts\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Sample the radiance field with the points along the rays.\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m sigma, rad \u001b[38;5;241m=\u001b[39m \u001b[43mradiance_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Reshape sigma and rad back to (batch_size, num_rays, self.n_samples, -1)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m sigma \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mview(batch_size, num_rays, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[85], line 115\u001b[0m, in \u001b[0;36mLatentRadField.forward\u001b[0;34m(self, xyz)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03mQueries the representation for the density and color values\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Do a forward pass through the scene representation.\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m features \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_rep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Do a forward pass through both the self.sigma and self.color MLPs\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# to yield sigma and color.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma(features)\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[85], line 37\u001b[0m, in \u001b[0;36mLatentHybridVoxelNeuralField.forward\u001b[0;34m(self, coordinate)\u001b[0m\n\u001b[1;32m     32\u001b[0m bs\u001b[38;5;241m=\u001b[39m coordinate\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Call self.grid to decode latent code into 3D grid.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Recall that you don't need to pass any input as the decoder is\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# evaluated on the latent which is a parameter in the LatentFeatureGrid.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(\"Coordinate: \", coordinate.min(), coordinate.max())\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Sample 3D grid at coordinate with grid_sample.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Remember to add dimensions to make it compatible with grid_sample\u001b[39;00m\n\u001b[1;32m     44\u001b[0m coord \u001b[38;5;241m=\u001b[39m coordinate[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,:,:]\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mLatentFeatureGrid.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mcoordinate: (batch_size, num_points, 2)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# return the output of the decoder on input of self.latent_grid.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mConvDecoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Call in_conv, hidden_conv, and out_conv modules on the input.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_conv(x)\n\u001b[1;32m    105\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_conv(x)\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/conv.py:607\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ubuntu/Data/tools/anaconda3/envs/mlig/lib/python3.9/site-packages/torch/nn/modules/conv.py:602\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    592\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    593\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 128, 3, 3, 3], expected input[1, 32, 128, 1, 1] to have 128 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mi, gt = next(iter(data_loader))\n",
    "\n",
    "    for key, val in mi.items():\n",
    "        print(key, val.shape)\n",
    "\n",
    "    test_out = srns_ad(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzWz_gQ2U-mi"
   },
   "source": [
    "No error --> looks good ;)\n",
    "\n",
    "We now need to adapt our auto-decoder loss from earlier to deal with the tuple output from our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZD7u99uXm8p"
   },
   "outputs": [],
   "source": [
    "def autodecoder_loss(mlp_out, gt, model):\n",
    "    (img, depth), latents = mlp_out # Have to unpack outputs of autodecoder wrapper\n",
    "\n",
    "    img_loss = ((img - gt)**2).mean()\n",
    "    latent_loss = (latents**2).mean()\n",
    "\n",
    "    param_loss = 0.\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'latent' not in name:\n",
    "            param_loss += (param**2).mean()\n",
    "    # return img_loss\n",
    "    return param_loss*1e-4 + latent_loss*1e-4 + img_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYgu0fn7ZZuj"
   },
   "source": [
    "We similarly need to adapt our `plot_output_ground_truth` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HS7832RzZfcs"
   },
   "outputs": [],
   "source": [
    "def plot_output_ground_truth(model_output, ground_truth, resolution):\n",
    "    (img_batch, depth_batch), latent = model_output\n",
    "\n",
    "    num_batch_items = 4\n",
    "    fig, axes = plt.subplots(num_batch_items, 3, figsize=(18, 6*num_batch_items), squeeze=False)\n",
    "\n",
    "    for i in range(num_batch_items):\n",
    "        img = img_batch[i]\n",
    "        depth = depth_batch[i]\n",
    "        gt = ground_truth[i]\n",
    "\n",
    "        axes[i, 0].imshow(img.cpu().view(*resolution).detach().numpy())\n",
    "        axes[i, 0].set_title(\"Trained MLP\")\n",
    "        axes[i, 1].imshow(gt.cpu().view(*resolution).detach().numpy())\n",
    "        axes[i, 1].set_title(\"Ground Truth\")\n",
    "\n",
    "        depth = depth.cpu().view(*resolution[:2]).detach().numpy()\n",
    "        axes[i, 2].imshow(depth, cmap='Greys')\n",
    "        axes[i, 2].set_title(\"Depth\")\n",
    "\n",
    "        for j in range(3):\n",
    "            axes[i, j].set_axis_off()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDNKz5uuDtu2"
   },
   "source": [
    "Let's fit!\n",
    "\n",
    "**Note: This will take quite a while to run, somewhere on the order of 15-20 minutes. You will not be graded based on how far this converges - we will judge whether your functions are correct or not based on unit-tests for each of the functions. This result here is just for you to see that it's working!**\n",
    "\n",
    "**Hint: If you get weird CUDA bugs, try your model out on CPU first! These are often indexing bugs that CUDA can offer limited insight on, but on CPU, the debugging messages will be much more meaningful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-uUB-u3gRbB"
   },
   "outputs": [],
   "source": [
    "srns_ad = srns_ad.cuda() # Copy the model to the GPU\n",
    "optim = torch.optim.Adam(lr=1e-4, params=srns_ad.parameters())\n",
    "\n",
    "fit(srns_ad,\n",
    "    iter(data_loader),\n",
    "    loss_fn=autodecoder_loss,\n",
    "    resolution=(img_sl, img_sl, 3),\n",
    "    plotting_function=plot_output_ground_truth,\n",
    "    total_steps=10_000,\n",
    "    optimizer=optim,\n",
    "    steps_til_summary=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34JPwKtl3oGh"
   },
   "source": [
    "You should observe the effect we discussed earlier: The model will converge to a soapbar-style \"average car\" after \\~500 steps, and only after training for a while longer (\\~2000 steps) will it start to form the correct shapes & colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP7W609XtLmX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFIFXslfixeN"
   },
   "source": [
    "## Part 2.6 Reconstructing an SRN car from a single image.\n",
    "\n",
    "With this latent space in hand, you are now able to reconstruct an SRN car from a **single** image - much in the same way as we reconstructed an MNIST digit from only half the image!\n",
    "\n",
    "This was first demonstrated in the paper \"Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations\" - with this course, you've basically caught up all the way (and much further) on what took Vincent 1.5 years :)\n",
    "\n",
    "As with the MNIST digit, let's pop a single image observation from our dataset and build a generator that only yields that one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nFVS_zxW_oQ"
   },
   "outputs": [],
   "source": [
    "img_sl = 64\n",
    "num_scenes = 1\n",
    "batch_size = 1\n",
    "\n",
    "dataset = SRNsCars(max_num_instances=num_scenes, img_sidelength=img_sl)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def test_generator():\n",
    "    sample = next(iter(data_loader))\n",
    "    while True:\n",
    "        yield sample\n",
    "\n",
    "test_dataset = test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL3t5IniYOEy"
   },
   "source": [
    "For the optimizer, we want to *only optimize the latent code - not the other parameters of the model*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHO04fDxYWaa"
   },
   "outputs": [],
   "source": [
    "srn_rf.rf.scene_rep.grid.latent_grid.data.normal_()\n",
    "\n",
    "optim = torch.optim.Adam(lr=1e-3,\n",
    "                         params=[srn_rf.rf.scene_rep.grid.latent_grid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4XRjZyOQJpf"
   },
   "source": [
    "We need to adapt our loss again, as this model does not return a latent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ffxuq9l5QNXh"
   },
   "outputs": [],
   "source": [
    "def test_rec_loss(mlp_out, gt, model):\n",
    "    (img, depth) = mlp_out # Have to unpack outputs of autodecoder wrapper\n",
    "\n",
    "    img_loss = ((img - gt)**2).mean()\n",
    "    return img_loss\n",
    "\n",
    "def test_plotting(model_output, ground_truth, resolution):\n",
    "    (img_batch, depth_batch) = model_output\n",
    "\n",
    "    num_batch_items = 1\n",
    "    fig, axes = plt.subplots(num_batch_items, 3, figsize=(18, 6*num_batch_items), squeeze=False)\n",
    "\n",
    "    i = 0\n",
    "    img = img_batch[i]\n",
    "    depth = depth_batch[i]\n",
    "    gt = ground_truth[i]\n",
    "\n",
    "    axes[i, 0].imshow(img.cpu().view(*resolution).detach().numpy())\n",
    "    axes[i, 0].set_title(\"Trained MLP\")\n",
    "    axes[i, 1].imshow(gt.cpu().view(*resolution).detach().numpy())\n",
    "    axes[i, 1].set_title(\"Ground Truth\")\n",
    "\n",
    "    depth = depth.cpu().view(*resolution[:2]).detach().numpy()\n",
    "    axes[i, 2].imshow(depth, cmap='Greys')\n",
    "    axes[i, 2].set_title(\"Depth\")\n",
    "\n",
    "    for j in range(3):\n",
    "        axes[i, j].set_axis_off()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnrDbue1Yd4Y"
   },
   "source": [
    "We can now reconstruct via fitting:\n",
    "\n",
    "**NOTE: the quality of this reconstruction is going to be somewhat poor, because we have only trained on a dataset of 100 cars. We would expect this to be *a lot* better if we had trained on more - in fact, this setup is very, very similar to that of \"Scene Representation Networks\", and with a bit of tweaking should yield similar results. If you have time and a bigger GPU, go ahead and try it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2pFcshAYfy5"
   },
   "outputs": [],
   "source": [
    "fit(srn_rf,\n",
    "    iter(test_dataset),\n",
    "    loss_fn=test_rec_loss,\n",
    "    resolution=(img_sl, img_sl, 3),\n",
    "    plotting_function=test_plotting,\n",
    "    total_steps=5_000,\n",
    "    optimizer=optim,\n",
    "    steps_til_summary=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hynr5QE5QsHN"
   },
   "source": [
    "# Rendering the car from multiple view points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he7Jyyb-Qve6"
   },
   "outputs": [],
   "source": [
    "f = h5py.File('cars_train.hdf5', 'r')\n",
    "\n",
    "instances = sorted(list(f.keys()))\n",
    "idx = 0\n",
    "\n",
    "key = instances[idx]\n",
    "instance = f[key]\n",
    "c2ws_ds = instance['pose']\n",
    "c2w_keys = list(c2ws_ds.keys())\n",
    "intrinsics = parse_intrinsics( instance['intrinsics.txt'] ).to(device)\n",
    "intrinsics[:2, :3] /= 128.\n",
    "\n",
    "x_pix = get_opencv_pixel_coordinates(128, 128)\n",
    "x_pix = x_pix[32:-32, 32:-32]\n",
    "\n",
    "x_pix = resize(x_pix,\n",
    "               (64, 64),\n",
    "               anti_aliasing=False,\n",
    "               order=0)\n",
    "x_pix = torch.tensor(x_pix).reshape(1, -1, 2).to(device)\n",
    "\n",
    "poses = []\n",
    "for i in range(len(c2w_keys)):\n",
    "    c2w = parse_pose( c2ws_ds[c2w_keys[i]] )\n",
    "    poses.append(c2w)\n",
    "\n",
    "\n",
    "poses = torch.stack(poses).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    frames = []\n",
    "    for i in range(len(poses)):\n",
    "        model_in = {'cam2world': poses[i:i+1], 'intrinsics': intrinsics[None, ...], 'x_pix': x_pix}\n",
    "        rgb, depth = srn_rf(model_in)\n",
    "\n",
    "        rgb = rgb.reshape(64, 64, 3).cpu().numpy()\n",
    "        frames.append(torch.tensor(rgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmPdOfqeQ4ly"
   },
   "outputs": [],
   "source": [
    "# We show a grid with the car viewed from multiple viewpoints even though\n",
    "# we only observed a single image of a car.\n",
    "output = torchvision.utils.make_grid(torch.stack(frames).permute(0, -1, 1, 2), 10)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(output.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYhYKoGOVfda"
   },
   "source": [
    "# The End\n",
    "This concludes Assignment 3. In summary, you have learned:\n",
    "\n",
    "\n",
    "1.   A convolutional decoder\n",
    "2.   How to use a convolutional decoder to decode a latent into a 2D or 3D grid\n",
    "3.   How to *manually* convert a single-latent module into an auto-decoder by swapping out an nn.Parameter with an nn.Embedding and modifying the forward pass\n",
    "4.   How to train a 2D auto-decoder on images.\n",
    "5.   How to use the trained auto-decoder for prior-based reconstruction from an incomplete observation.\n",
    "6.   How to use a 3D convolutional decoder to build a hybrid discrete-continuous neural field based on a voxelgrid.\n",
    "7.   How to convert that into an Auto-Decoder by writing a wrapper function instead of manually editing.\n",
    "8.   How to train a latent space of **3D Scenes** solely via differentiable rendering.\n",
    "9.   How to use that latent space for **single-image 3D reconstruction**,thereby reproducing the key result from the paper \"Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR2KuxuaVpFX"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "1.   Click \"Runtime -> Run all\" and make sure all cells run without an error (you can tell by whether the *final* cell was executed. You can then be confident that you have implemented everything you were supposed to implement, or at least deleted the \"raise NotImplementedError\".\n",
    "2.   Once you're certain, click \"File -> Download -> Download .ipynb\" and \"File -> Download -> Download .py\"\n",
    "3.   Log in to Canvas\n",
    "4.   Upload the `.py` and `.ipynb` file in a `.zip` archive with the filename being your Kerberos ID (e.g. `prafull.zip`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYUwjpEpVpXf"
   },
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
